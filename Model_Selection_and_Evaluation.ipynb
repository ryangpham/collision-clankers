{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c594b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration overrides from Stage 1/2 notebooks\n",
    "# Set dataset path, target, and task type based on What_should_we_keep.ipynb\n",
    "\n",
    "import os\n",
    "\n",
    "data_path = 'data/US_Accidents_March23.csv'\n",
    "target_column = 'Severity'\n",
    "task_type = 'classification'\n",
    "primary_metric = 'f1'\n",
    "\n",
    "# Stage 2 artifacts\n",
    "use_removed_columns_file = True\n",
    "removed_cols_path = os.path.join('outputs', 'removed_columns_stage2.json')\n",
    "selected_doc_path = os.path.join('outputs', 'selected_features_doc.csv')\n",
    "use_selected_doc = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime controls\n",
    "MAX_ROWS = 200_000  # sample size cap\n",
    "SAMPLE_RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a7b7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Install required packages (skip if already installed)\n",
    "import sys, subprocess\n",
    "pkgs = [\"pandas\", \"numpy\", \"scikit-learn\", \"matplotlib\", \"seaborn\", \"joblib\"]\n",
    "try:\n",
    "    import sklearn, pandas, numpy\n",
    "except Exception:\n",
    "    for p in pkgs:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p])\n",
    "\n",
    "\n",
    "# Imports and configuration\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix,\n",
    "    mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# Models\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier, XGBRegressor  # optional\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "\n",
    "# Metric helpers\n",
    "def compute_classification_metrics(y_true, y_pred, y_proba=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    roc = None\n",
    "    try:\n",
    "        if y_proba is not None:\n",
    "            if len(np.unique(y_true)) == 2:\n",
    "                roc = roc_auc_score(y_true, y_proba[:, 1])\n",
    "            else:\n",
    "                roc = roc_auc_score(y_true, y_proba, multi_class=\"ovr\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"roc_auc\": roc, \"confusion_matrix\": cm}\n",
    "\n",
    "\n",
    "def compute_regression_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\"mae\": mae, \"mse\": mse, \"rmse\": rmse, \"r2\": r2}\n",
    "\n",
    "\n",
    "classification_scorers = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"f1\": make_scorer(lambda yt, yp: precision_recall_fscore_support(yt, yp, average=\"weighted\")[2]),\n",
    "}\n",
    "regression_scorers = {\n",
    "    \"rmse\": make_scorer(lambda yt, yp: np.sqrt(mean_squared_error(yt, yp)), greater_is_better=False),\n",
    "    \"r2\": make_scorer(r2_score),\n",
    "}\n",
    "\n",
    "\n",
    "# Custom metrics\n",
    "from sklearn.metrics import fbeta_score\n",
    "def fbeta_weighted(y_true, y_pred, beta=2):\n",
    "    return fbeta_score(y_true, y_pred, beta=beta, average=\"weighted\")\n",
    "custom_classification_scorer = make_scorer(lambda yt, yp: fbeta_weighted(yt, yp, beta=2))\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred))\n",
    "    denom = np.where(denom == 0, 1, denom)\n",
    "    return np.mean(2.0 * np.abs(y_pred - y_true) / denom)\n",
    "custom_regression_scorer = make_scorer(smape, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654ab2b",
   "metadata": {},
   "source": [
    "# Phase 4: Model Selection and Evaluation\n",
    "\n",
    "This notebook implements training, evaluation, custom metrics, cross-validation, hyperparameter optimization, and model comparison across multiple model families. Configure the task type and dataset path in Cell 1. Stage 2 selections are applied automatically from `outputs/selected_features_doc.csv` or `outputs/removed_columns_stage2.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c7543b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: Apply Stage 2 selection using documentation CSV or removed-columns JSON\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "def apply_stage2_selection(df: pd.DataFrame, target: str,\n",
    "                           selected_doc_csv: str = selected_doc_path,\n",
    "                           removed_json: str = removed_cols_path,\n",
    "                           prefer_doc: bool = use_selected_doc):\n",
    "    df2 = df.copy()\n",
    "    if prefer_doc and os.path.exists(selected_doc_csv):\n",
    "        doc = pd.read_csv(selected_doc_csv)\n",
    "        kept = doc.loc[doc['Kept?'].str.lower() == 'yes', 'Feature'].tolist()\n",
    "        # Ensure target present\n",
    "        if target not in kept:\n",
    "            kept.append(target)\n",
    "        kept = [c for c in kept if c in df2.columns]\n",
    "        print(f\"Using selected_features_doc.csv; keeping {len(kept)} columns.\")\n",
    "        return df2[kept]\n",
    "    # Fallback to removed-columns file\n",
    "    if os.path.exists(removed_json):\n",
    "        with open(removed_json, 'r') as f:\n",
    "            removed_sets = json.load(f)\n",
    "        drop_cols = []\n",
    "        for _, cols in removed_sets.items():\n",
    "            drop_cols.extend([c for c in cols if c in df2.columns])\n",
    "        if drop_cols:\n",
    "            print(f\"Dropping {len(drop_cols)} columns from removed_columns_stage2.json\")\n",
    "            df2 = df2.drop(columns=list(set(drop_cols)))\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88f0f7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 14 columns from removed_columns_stage2.json\n",
      "Sampled 300000 rows from 7728394.\n",
      "Sampled 300000 rows from 7728394.\n",
      "Numeric features: 8 | Categorical features: 23\n",
      "Numeric features: 8 | Categorical features: 23\n"
     ]
    }
   ],
   "source": [
    "# Reload data using Stage 2 selection before splitting, with sampling\n",
    "assert os.path.exists(data_path), f\"Data file not found: {data_path}. Please set data_path.\"\n",
    "\n",
    "df_full = pd.read_csv(data_path)\n",
    "df_full = apply_stage2_selection(df_full, target_column)\n",
    "\n",
    "# Sample to MAX_ROWS to speed up training\n",
    "if len(df_full) > MAX_ROWS:\n",
    "    # Stratified sample for classification, random sample for regression\n",
    "    if task_type == \"classification\":\n",
    "        df = df_full.groupby(target_column, group_keys=False).apply(lambda g: g.sample(frac=min(1.0, MAX_ROWS/len(df_full)), random_state=SAMPLE_RANDOM_STATE))\n",
    "    else:\n",
    "        df = df_full.sample(n=MAX_ROWS, random_state=SAMPLE_RANDOM_STATE)\n",
    "    print(f\"Sampled {len(df)} rows from {len(df_full)}.\")\n",
    "else:\n",
    "    df = df_full.copy()\n",
    "    print(f\"Using full dataset with {len(df)} rows.\")\n",
    "\n",
    "assert target_column in df.columns, f\"Target column '{target_column}' not in data.\"\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n",
    "# Re-identify column types after selection\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(f\"Numeric features: {len(numeric_features)} | Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Split\n",
    "if task_type == \"classification\":\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e5bc2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Train shape: (240000, 640494) Test shape: (60000, 640494)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess once and cache transformed arrays\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on train, transform train and test once\n",
    "X_train_t = preprocessor.fit_transform(X_train)\n",
    "X_test_t = preprocessor.transform(X_test)\n",
    "\n",
    "# Feature names after encoding\n",
    "try:\n",
    "    feature_names = []\n",
    "    # numeric\n",
    "    feature_names += numeric_features\n",
    "    # categorical via OneHot\n",
    "    ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "    ohe_names = list(ohe.get_feature_names_out(categorical_features))\n",
    "    feature_names += ohe_names\n",
    "except Exception:\n",
    "    feature_names = [f\"f{i}\" for i in range(X_train_t.shape[1])]\n",
    "\n",
    "print(\"Preprocessing complete. Train shape:\", X_train_t.shape, \"Test shape:\", X_test_t.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline and core model families (using preprocessed arrays)\n",
    "results = []\n",
    "\n",
    "if task_type == \"classification\":\n",
    "    # Baseline: most frequent\n",
    "    baseline = DummyClassifier(strategy=\"most_frequent\", random_state=RANDOM_STATE)\n",
    "    baseline.fit(X_train_t, y_train)\n",
    "    y_pred = baseline.predict(X_test_t)\n",
    "    metrics = compute_classification_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"Baseline(DummyClassifier)\", **metrics, \"fit_time\": None, \"predict_time\": None})\n",
    "\n",
    "    # Decision Tree\n",
    "    dt = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "    start = time.perf_counter(); dt.fit(X_train_t, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = dt.predict(X_test_t); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_classification_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"DecisionTreeClassifier\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "    # KNN\n",
    "    knn = KNeighborsClassifier()\n",
    "    start = time.perf_counter(); knn.fit(X_train_t, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = knn.predict(X_test_t); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_classification_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"KNeighborsClassifier\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "    # Naive Bayes\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train_t.toarray() if hasattr(X_train_t, 'toarray') else X_train_t, y_train)\n",
    "    y_pred = nb.predict(X_test_t.toarray() if hasattr(X_test_t, 'toarray') else X_test_t)\n",
    "    metrics = compute_classification_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"GaussianNB\", **metrics, \"fit_time\": None, \"predict_time\": None})\n",
    "\n",
    "    # Logistic Regression\n",
    "    logr = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "    start = time.perf_counter(); logr.fit(X_train_t, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = logr.predict(X_test_t); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_classification_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"LogisticRegression\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "    # SVM\n",
    "    svm_model = SVC(probability=True, random_state=RANDOM_STATE)\n",
    "    start = time.perf_counter(); svm_model.fit(X_train_t, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = svm_model.predict(X_test_t); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_classification_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"SVC\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE)\n",
    "    start = time.perf_counter(); rf.fit(X_train_t, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = rf.predict(X_test_t); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_classification_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"RandomForestClassifier\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "else:\n",
    "    baseline = DummyRegressor(strategy=\"mean\")\n",
    "    baseline.fit(X_train_t, y_train)\n",
    "    y_pred = baseline.predict(X_test_t)\n",
    "    metrics = compute_regression_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"Baseline(DummyRegressor)\", **metrics, \"fit_time\": None, \"predict_time\": None})\n",
    "\n",
    "    # Decision Tree\n",
    "    dt = DecisionTreeRegressor(random_state=RANDOM_STATE)\n",
    "    start = time.perf_counter(); dt.fit(X_train_t, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = dt.predict(X_test_t); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_regression_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"DecisionTreeRegressor\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "    # KNN\n",
    "    knn = KNeighborsRegressor()\n",
    "    start = time.perf_counter(); knn.fit(X_train_t, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = knn.predict(X_test_t); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_regression_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"KNeighborsRegressor\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "    # Linear Regression\n",
    "    lr = LinearRegression()\n",
    "    start = time.perf_counter(); lr.fit(X_train_t, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = lr.predict(X_test_t); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_regression_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"LinearRegression\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "    # SVR\n",
    "    svm_model = SVR()\n",
    "    start = time.perf_counter(); svm_model.fit(X_train_t, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = svm_model.predict(X_test_t); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_regression_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"SVR\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestRegressor(n_estimators=200, random_state=RANDOM_STATE)\n",
    "    start = time.perf_counter(); rf.fit(X_train_t, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = rf.predict(X_test_t); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_regression_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"RandomForestRegressor\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad6db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation and Grid Search (optional, uses pipelines to avoid leakage)\n",
    "# For speed with large data, consider reducing n_splits or skipping CV.\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE) if task_type == \"classification\" else KFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "best_models = []\n",
    "\n",
    "if task_type == \"classification\":\n",
    "    param_grids = {\n",
    "        \"LogisticRegression\": {\"clf__C\": [0.5, 1.0], \"clf__solver\": [\"lbfgs\"]},\n",
    "        \"SVC\": {\"clf__kernel\": [\"rbf\"], \"clf__C\": [1], \"clf__gamma\": [\"scale\"]},\n",
    "        \"RandomForestClassifier\": {\"clf__n_estimators\": [100], \"clf__max_depth\": [None, 20]},\n",
    "    }\n",
    "    base_models = {\n",
    "        \"LogisticRegression\": Pipeline([(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression(max_iter=500, random_state=RANDOM_STATE))]),\n",
    "        \"SVC\": Pipeline([(\"preprocessor\", preprocessor), (\"clf\", SVC(probability=True, random_state=RANDOM_STATE))]),\n",
    "        \"RandomForestClassifier\": Pipeline([(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier(random_state=RANDOM_STATE))]),\n",
    "    }\n",
    "    scoring = classification_scorers[\"f1\"]\n",
    "else:\n",
    "    param_grids = {\n",
    "        \"SVR\": {\"reg__kernel\": [\"rbf\"], \"reg__C\": [1], \"reg__gamma\": [\"scale\"]},\n",
    "        \"RandomForestRegressor\": {\"reg__n_estimators\": [100], \"reg__max_depth\": [None, 20]},\n",
    "        \"LinearRegression\": {},\n",
    "    }\n",
    "    base_models = {\n",
    "        \"SVR\": Pipeline([(\"preprocessor\", preprocessor), (\"reg\", SVR())]),\n",
    "        \"RandomForestRegressor\": Pipeline([(\"preprocessor\", preprocessor), (\"reg\", RandomForestRegressor(random_state=RANDOM_STATE))]),\n",
    "        \"LinearRegression\": Pipeline([(\"preprocessor\", preprocessor), (\"reg\", LinearRegression())]),\n",
    "    }\n",
    "    scoring = regression_scorers[\"rmse\"]\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    grid = param_grids.get(name, {})\n",
    "    gs = GridSearchCV(model, grid, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_models.append({\n",
    "        \"name\": name,\n",
    "        \"best_estimator\": gs.best_estimator_,\n",
    "        \"best_params\": gs.best_params_,\n",
    "        \"best_score_cv\": gs.best_score_,\n",
    "    })\n",
    "\n",
    "best_models_df = pd.DataFrame([{ \"model\": bm[\"name\"], \"best_score_cv\": bm[\"best_score_cv\"], \"best_params\": bm[\"best_params\"] } for bm in best_models])\n",
    "best_models_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7295400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaderboard, plots, and save best estimator\n",
    "comp_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "if task_type == \"classification\":\n",
    "    sns.barplot(data=comp_df, x=\"model\", y=\"f1\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "else:\n",
    "    sns.barplot(data=comp_df, x=\"model\", y=\"rmse\")\n",
    "    plt.ylabel(\"RMSE (lower is better)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Primary Metric by Model\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Rank and justification\n",
    "if task_type == \"classification\":\n",
    "    ranked = comp_df.sort_values(by=\"f1\", ascending=False)\n",
    "    best_row = ranked.iloc[0]\n",
    "    justification = (\n",
    "        f\"Selected {best_row['model']} for highest F1 ({best_row['f1']:.3f}). \"\n",
    "        f\"Compared to baseline accuracy {comp_df.loc[comp_df['model']=='Baseline(DummyClassifier)','accuracy'].max():.3f} where available. \"\n",
    "        f\"Balance of performance, interpretability, and efficiency.\")\n",
    "else:\n",
    "    ranked = comp_df.sort_values(by=\"rmse\", ascending=True)\n",
    "    best_row = ranked.iloc[0]\n",
    "    justification = (\n",
    "        f\"Selected {best_row['model']} for lowest RMSE ({best_row['rmse']:.3f}) and strong R2 ({best_row.get('r2', np.nan):.3f}). \"\n",
    "        f\"Improves over baseline.\")\n",
    "\n",
    "\n",
    "print(justification)\n",
    "\n",
    "\n",
    "import os\n",
    "from joblib import dump\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "comp_df.to_csv(os.path.join(\"outputs\", \"model_leaderboard.csv\"), index=False)\n",
    "ranked.to_csv(os.path.join(\"outputs\", \"model_leaderboard_ranked.csv\"), index=False)\n",
    "\n",
    "\n",
    "# Save best estimator from grid search\n",
    "if len(best_models) > 0:\n",
    "    if task_type == \"classification\":\n",
    "        best_from_grid = max(best_models, key=lambda bm: bm[\"best_score_cv\"])\n",
    "    else:\n",
    "        best_from_grid = min(best_models, key=lambda bm: bm[\"best_score_cv\"])\n",
    "    best_est = best_from_grid[\"best_estimator\"]\n",
    "    best_est.fit(X_train, y_train)\n",
    "    dump(best_est, os.path.join(\"outputs\", \"best_model.pkl\"))\n",
    "    print(\"Saved best tuned model:\", best_from_grid[\"name\"], best_from_grid[\"best_params\"])\n",
    "\n",
    "\n",
    "# Save metrics summary\n",
    "summary = {\"task_type\": task_type, \"primary_metric\": primary_metric}\n",
    "with open(os.path.join(\"outputs\", \"metrics_summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
