{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c594b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration overrides from Stage 1/2 notebooks\n",
    "# Set dataset path, target, and task type based on What_should_we_keep.ipynb\n",
    "\n",
    "import os\n",
    "\n",
    "data_path = 'data/US_Accidents_March23.csv'\n",
    "target_column = 'Severity'\n",
    "task_type = 'classification'\n",
    "primary_metric = 'f1'\n",
    "\n",
    "# Stage 2 artifacts\n",
    "use_removed_columns_file = True\n",
    "removed_cols_path = os.path.join('outputs', 'removed_columns_stage2.json')\n",
    "selected_doc_path = os.path.join('outputs', 'selected_features_doc.csv')\n",
    "use_selected_doc = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a7b7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Install required packages (skip if already installed)\n",
    "import sys, subprocess\n",
    "pkgs = [\"pandas\", \"numpy\", \"scikit-learn\", \"matplotlib\", \"seaborn\", \"joblib\"]\n",
    "try:\n",
    "    import sklearn, pandas, numpy\n",
    "except Exception:\n",
    "    for p in pkgs:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p])\n",
    "\n",
    "\n",
    "# Imports and configuration\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix,\n",
    "    mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# Models\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier, XGBRegressor  # optional\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "\n",
    "# Metric helpers\n",
    "def compute_classification_metrics(y_true, y_pred, y_proba=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    roc = None\n",
    "    try:\n",
    "        if y_proba is not None:\n",
    "            if len(np.unique(y_true)) == 2:\n",
    "                roc = roc_auc_score(y_true, y_proba[:, 1])\n",
    "            else:\n",
    "                roc = roc_auc_score(y_true, y_proba, multi_class=\"ovr\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"roc_auc\": roc, \"confusion_matrix\": cm}\n",
    "\n",
    "\n",
    "def compute_regression_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\"mae\": mae, \"mse\": mse, \"rmse\": rmse, \"r2\": r2}\n",
    "\n",
    "\n",
    "classification_scorers = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"f1\": make_scorer(lambda yt, yp: precision_recall_fscore_support(yt, yp, average=\"weighted\")[2]),\n",
    "}\n",
    "regression_scorers = {\n",
    "    \"rmse\": make_scorer(lambda yt, yp: np.sqrt(mean_squared_error(yt, yp)), greater_is_better=False),\n",
    "    \"r2\": make_scorer(r2_score),\n",
    "}\n",
    "\n",
    "\n",
    "# Custom metrics\n",
    "from sklearn.metrics import fbeta_score\n",
    "def fbeta_weighted(y_true, y_pred, beta=2):\n",
    "    return fbeta_score(y_true, y_pred, beta=beta, average=\"weighted\")\n",
    "custom_classification_scorer = make_scorer(lambda yt, yp: fbeta_weighted(yt, yp, beta=2))\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred))\n",
    "    denom = np.where(denom == 0, 1, denom)\n",
    "    return np.mean(2.0 * np.abs(y_pred - y_true) / denom)\n",
    "custom_regression_scorer = make_scorer(smape, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654ab2b",
   "metadata": {},
   "source": [
    "# Phase 4: Model Selection and Evaluation\n",
    "\n",
    "This notebook implements training, evaluation, custom metrics, cross-validation, hyperparameter optimization, and model comparison across multiple model families. Configure the task type and dataset path in Cell 1. Stage 2 selections are applied automatically from `outputs/selected_features_doc.csv` or `outputs/removed_columns_stage2.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c7543b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: Apply Stage 2 selection using documentation CSV or removed-columns JSON\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "def apply_stage2_selection(df: pd.DataFrame, target: str,\n",
    "                           selected_doc_csv: str = selected_doc_path,\n",
    "                           removed_json: str = removed_cols_path,\n",
    "                           prefer_doc: bool = use_selected_doc):\n",
    "    df2 = df.copy()\n",
    "    if prefer_doc and os.path.exists(selected_doc_csv):\n",
    "        doc = pd.read_csv(selected_doc_csv)\n",
    "        kept = doc.loc[doc['Kept?'].str.lower() == 'yes', 'Feature'].tolist()\n",
    "        # Ensure target present\n",
    "        if target not in kept:\n",
    "            kept.append(target)\n",
    "        kept = [c for c in kept if c in df2.columns]\n",
    "        print(f\"Using selected_features_doc.csv; keeping {len(kept)} columns.\")\n",
    "        return df2[kept]\n",
    "    # Fallback to removed-columns file\n",
    "    if os.path.exists(removed_json):\n",
    "        with open(removed_json, 'r') as f:\n",
    "            removed_sets = json.load(f)\n",
    "        drop_cols = []\n",
    "        for _, cols in removed_sets.items():\n",
    "            drop_cols.extend([c for c in cols if c in df2.columns])\n",
    "        if drop_cols:\n",
    "            print(f\"Dropping {len(drop_cols)} columns from removed_columns_stage2.json\")\n",
    "            df2 = df2.drop(columns=list(set(drop_cols)))\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88f0f7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 14 columns from removed_columns_stage2.json\n",
      "Numeric features: 8 | Categorical features: 23\n",
      "Numeric features: 8 | Categorical features: 23\n"
     ]
    }
   ],
   "source": [
    "# Reload data using Stage 2 selection before splitting\n",
    "assert os.path.exists(data_path), f\"Data file not found: {data_path}. Please set data_path.\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df = apply_stage2_selection(df, target_column)\n",
    "\n",
    "assert target_column in df.columns, f\"Target column '{target_column}' not in data.\"\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n",
    "# Re-identify column types after selection\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(f\"Numeric features: {len(numeric_features)} | Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Split\n",
    "if task_type == \"classification\":\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e5bc2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline and core model families\n",
    "results = []\n",
    "\n",
    "\n",
    "if task_type == \"classification\":\n",
    "    baseline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", DummyClassifier(strategy=\"most_frequent\", random_state=RANDOM_STATE))])\n",
    "    baseline.fit(X_train, y_train)\n",
    "    y_pred = baseline.predict(X_test)\n",
    "    metrics = compute_classification_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"Baseline(DummyClassifier)\", **metrics, \"fit_time\": None, \"predict_time\": None})\n",
    "\n",
    "\n",
    "    # Decision Tree\n",
    "    dt = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", DecisionTreeClassifier(random_state=RANDOM_STATE))])\n",
    "    start = time.perf_counter(); dt.fit(X_train, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = dt.predict(X_test); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_classification_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"DecisionTreeClassifier\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "\n",
    "    # KNN\n",
    "    knn = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", KNeighborsClassifier())])\n",
    "    start = time.perf_counter(); knn.fit(X_train, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = knn.predict(X_test); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_classification_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"KNeighborsClassifier\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "\n",
    "    # Naive Bayes\n",
    "    nb = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", GaussianNB())])\n",
    "    nb.fit(X_train, y_train)\n",
    "    y_pred = nb.predict(X_test)\n",
    "    metrics = compute_classification_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"GaussianNB\", **metrics, \"fit_time\": None, \"predict_time\": None})\n",
    "\n",
    "\n",
    "    # Logistic Regression\n",
    "    logr = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))])\n",
    "    start = time.perf_counter(); logr.fit(X_train, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = logr.predict(X_test); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_classification_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"LogisticRegression\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "\n",
    "    # SVM\n",
    "    svm_model = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC(probability=True, random_state=RANDOM_STATE))])\n",
    "    start = time.perf_counter(); svm_model.fit(X_train, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = svm_model.predict(X_test); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_classification_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"SVC\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "\n",
    "    # Random Forest\n",
    "    rf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE))])\n",
    "    start = time.perf_counter(); rf.fit(X_train, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = rf.predict(X_test); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_classification_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"RandomForestClassifier\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "\n",
    "else:\n",
    "    baseline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"reg\", DummyRegressor(strategy=\"mean\"))])\n",
    "    baseline.fit(X_train, y_train)\n",
    "    y_pred = baseline.predict(X_test)\n",
    "    metrics = compute_regression_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"Baseline(DummyRegressor)\", **metrics, \"fit_time\": None, \"predict_time\": None})\n",
    "\n",
    "\n",
    "    # Decision Tree\n",
    "    dt = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"reg\", DecisionTreeRegressor(random_state=RANDOM_STATE))])\n",
    "    start = time.perf_counter(); dt.fit(X_train, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = dt.predict(X_test); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_regression_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"DecisionTreeRegressor\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "\n",
    "    # KNN\n",
    "    knn = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"reg\", KNeighborsRegressor())])\n",
    "    start = time.perf_counter(); knn.fit(X_train, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = knn.predict(X_test); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_regression_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"KNeighborsRegressor\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "\n",
    "    # Linear Regression\n",
    "    lr = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"reg\", LinearRegression())])\n",
    "    start = time.perf_counter(); lr.fit(X_train, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = lr.predict(X_test); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_regression_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"LinearRegression\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "\n",
    "    # SVR\n",
    "    svm_model = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"reg\", SVR())])\n",
    "    start = time.perf_counter(); svm_model.fit(X_train, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = svm_model.predict(X_test); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_regression_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"SVR\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})\n",
    "\n",
    "\n",
    "    # Random Forest\n",
    "    rf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"reg\", RandomForestRegressor(n_estimators=200, random_state=RANDOM_STATE))])\n",
    "    start = time.perf_counter(); rf.fit(X_train, y_train); fit_t = time.perf_counter()-start\n",
    "    start = time.perf_counter(); y_pred = rf.predict(X_test); pred_t = time.perf_counter()-start\n",
    "    metrics = compute_regression_metrics(y_test, y_pred)\n",
    "    results.append({\"model\": \"RandomForestRegressor\", **metrics, \"fit_time\": fit_t, \"predict_time\": pred_t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad6db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation and Grid Search\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE) if task_type == \"classification\" else KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "if task_type == \"classification\":\n",
    "    param_grids = {\n",
    "        \"DecisionTreeClassifier\": {\"clf__max_depth\": [None, 5, 10], \"clf__min_samples_split\": [2, 5, 10]},\n",
    "        \"KNeighborsClassifier\": {\"clf__n_neighbors\": [5, 11, 21], \"clf__weights\": [\"uniform\", \"distance\"]},\n",
    "        \"LogisticRegression\": {\"clf__C\": [0.1, 1.0, 10.0], \"clf__penalty\": [\"l2\"], \"clf__solver\": [\"lbfgs\", \"saga\"]},\n",
    "        \"SVC\": {\"clf__kernel\": [\"rbf\", \"linear\"], \"clf__C\": [0.5, 1, 2], \"clf__gamma\": [\"scale\", \"auto\"]},\n",
    "        \"RandomForestClassifier\": {\"clf__n_estimators\": [100, 200], \"clf__max_depth\": [None, 10, 20], \"clf__max_features\": [\"sqrt\", \"log2\"]},\n",
    "    }\n",
    "    base_models = {\n",
    "        \"DecisionTreeClassifier\": Pipeline([(\"preprocessor\", preprocessor), (\"clf\", DecisionTreeClassifier(random_state=RANDOM_STATE))]),\n",
    "        \"KNeighborsClassifier\": Pipeline([(\"preprocessor\", preprocessor), (\"clf\", KNeighborsClassifier())]),\n",
    "        \"LogisticRegression\": Pipeline([(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))]),\n",
    "        \"SVC\": Pipeline([(\"preprocessor\", preprocessor), (\"clf\", SVC(probability=True, random_state=RANDOM_STATE))]),\n",
    "        \"RandomForestClassifier\": Pipeline([(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier(random_state=RANDOM_STATE))]),\n",
    "    }\n",
    "    scoring = classification_scorers[\"f1\"]\n",
    "else:\n",
    "    param_grids = {\n",
    "        \"DecisionTreeRegressor\": {\"reg__max_depth\": [None, 5, 10], \"reg__min_samples_split\": [2, 5, 10]},\n",
    "        \"KNeighborsRegressor\": {\"reg__n_neighbors\": [5, 11, 21], \"reg__weights\": [\"uniform\", \"distance\"]},\n",
    "        \"SVR\": {\"reg__kernel\": [\"rbf\", \"linear\"], \"reg__C\": [0.5, 1, 2], \"reg__gamma\": [\"scale\", \"auto\"]},\n",
    "        \"RandomForestRegressor\": {\"reg__n_estimators\": [100, 200], \"reg__max_depth\": [None, 10, 20], \"reg__max_features\": [\"sqrt\", \"log2\"]},\n",
    "        \"LinearRegression\": {},\n",
    "    }\n",
    "    base_models = {\n",
    "        \"DecisionTreeRegressor\": Pipeline([(\"preprocessor\", preprocessor), (\"reg\", DecisionTreeRegressor(random_state=RANDOM_STATE))]),\n",
    "        \"KNeighborsRegressor\": Pipeline([(\"preprocessor\", preprocessor), (\"reg\", KNeighborsRegressor())]),\n",
    "        \"SVR\": Pipeline([(\"preprocessor\", preprocessor), (\"reg\", SVR())]),\n",
    "        \"RandomForestRegressor\": Pipeline([(\"preprocessor\", preprocessor), (\"reg\", RandomForestRegressor(random_state=RANDOM_STATE))]),\n",
    "        \"LinearRegression\": Pipeline([(\"preprocessor\", preprocessor), (\"reg\", LinearRegression())]),\n",
    "    }\n",
    "    scoring = regression_scorers[\"rmse\"]\n",
    "\n",
    "\n",
    "best_models = []\n",
    "for name, model in base_models.items():\n",
    "    grid = param_grids.get(name, {})\n",
    "    gs = GridSearchCV(model, grid, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_models.append({\n",
    "        \"name\": name,\n",
    "        \"best_estimator\": gs.best_estimator_,\n",
    "        \"best_params\": gs.best_params_,\n",
    "        \"best_score_cv\": gs.best_score_,\n",
    "    })\n",
    "\n",
    "\n",
    "best_models_df = pd.DataFrame([{ \"model\": bm[\"name\"], \"best_score_cv\": bm[\"best_score_cv\"], \"best_params\": bm[\"best_params\"] } for bm in best_models])\n",
    "best_models_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7295400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaderboard, plots, and save best estimator\n",
    "comp_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "if task_type == \"classification\":\n",
    "    sns.barplot(data=comp_df, x=\"model\", y=\"f1\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "else:\n",
    "    sns.barplot(data=comp_df, x=\"model\", y=\"rmse\")\n",
    "    plt.ylabel(\"RMSE (lower is better)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Primary Metric by Model\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Rank and justification\n",
    "if task_type == \"classification\":\n",
    "    ranked = comp_df.sort_values(by=\"f1\", ascending=False)\n",
    "    best_row = ranked.iloc[0]\n",
    "    justification = (\n",
    "        f\"Selected {best_row['model']} for highest F1 ({best_row['f1']:.3f}). \"\n",
    "        f\"Compared to baseline accuracy {comp_df.loc[comp_df['model']=='Baseline(DummyClassifier)','accuracy'].max():.3f} where available. \"\n",
    "        f\"Balance of performance, interpretability, and efficiency.\")\n",
    "else:\n",
    "    ranked = comp_df.sort_values(by=\"rmse\", ascending=True)\n",
    "    best_row = ranked.iloc[0]\n",
    "    justification = (\n",
    "        f\"Selected {best_row['model']} for lowest RMSE ({best_row['rmse']:.3f}) and strong R2 ({best_row.get('r2', np.nan):.3f}). \"\n",
    "        f\"Improves over baseline.\")\n",
    "\n",
    "\n",
    "print(justification)\n",
    "\n",
    "\n",
    "import os\n",
    "from joblib import dump\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "comp_df.to_csv(os.path.join(\"outputs\", \"model_leaderboard.csv\"), index=False)\n",
    "ranked.to_csv(os.path.join(\"outputs\", \"model_leaderboard_ranked.csv\"), index=False)\n",
    "\n",
    "\n",
    "# Save best estimator from grid search\n",
    "if len(best_models) > 0:\n",
    "    if task_type == \"classification\":\n",
    "        best_from_grid = max(best_models, key=lambda bm: bm[\"best_score_cv\"])\n",
    "    else:\n",
    "        best_from_grid = min(best_models, key=lambda bm: bm[\"best_score_cv\"])\n",
    "    best_est = best_from_grid[\"best_estimator\"]\n",
    "    best_est.fit(X_train, y_train)\n",
    "    dump(best_est, os.path.join(\"outputs\", \"best_model.pkl\"))\n",
    "    print(\"Saved best tuned model:\", best_from_grid[\"name\"], best_from_grid[\"best_params\"])\n",
    "\n",
    "\n",
    "# Save metrics summary\n",
    "summary = {\"task_type\": task_type, \"primary_metric\": primary_metric}\n",
    "with open(os.path.join(\"outputs\", \"metrics_summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
